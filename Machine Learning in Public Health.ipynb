{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Public Health Workshop\n",
    "\n",
    "I'm going to walk through how we might work towards building a first machine learning model. But first, let's briefly discuss what machine learning is. This is going to be a rough crash course -- ask questions if you feel there's a gap!\n",
    "\n",
    "\n",
    "**Disclaimer: this is a super fast crash course. In practice, applying data science to public health is challenging. It's important problem but it requires deep collaboration between domain experts in the health field and data scientists to build valuable solutions. Take this knowledge to collaborate with domain experts -- not to \"disrupt\" their industry.**\n",
    "\n",
    "\n",
    "## Motivation for Machine Learning\n",
    "For many problems, it's sufficient to write a deterministic program to solve it. For example, if you want to find the sum of all numbers between 1 and 1 million, we could write a python program such as:\n",
    "\n",
    "```python\n",
    "sum([i for i in range(1000000)])\n",
    "```\n",
    "\n",
    "However, there are some problems that are more difficult to solve via computing. For example, how would you write a program to detect a face? This is a task that seems easy for most people -- easy to the eXvant that it is unconscious. But writing a program to perform such a task is difficult!\n",
    "\n",
    "This difficulty is the motivation for machine learning.\n",
    "\n",
    "## What is Machine Learning?\n",
    "If we don't know the process for detecting faces, what if we learn that process? Let's frame our facial detection problem a bit:\n",
    "\n",
    "Given an image, we want to determine whether or not a face exists in that image.\n",
    "\n",
    "Assuming we've already given up on directly programming the process of detecting a face, how might we go about learning that process?\n",
    "\n",
    "In machine learning, we typically give some data to an algorithm for \"learning\" and then you can use what that algorithm learned to make predictions about new data.\n",
    "\n",
    "In the facial detection example, we might provide many images, each with a label - \"Yes\" if it contains a face or \"No\" if it does not (in practice this could be 1 and 0, respectively).\n",
    "\n",
    "This data is used to _train_ a machine learning model.\n",
    "\n",
    "## Model Training\n",
    "We call the process of a model learning from data \"training.\" In this, we typically give a model some data (often with the correct answers -- hence the label) and let it tweak itself according to some algorithm to get answers right more often. Think of this as homework: you often have some answers for homework questions so that you can try the problem yourself and then score yourself to see how well you did. This let's you improve your own score before it really counts (i.e. on the test).\n",
    "\n",
    "In our facial detection example, you would give the model an image and let the model predict whether or not a face was in the image. Then you'd score the model prediction with the real label (we'll talk about how we score in a bit). The training algorithm will then tweak the model in light of a this score to do more of the things that get right answers and less of the things that get wrong answers.\n",
    "\n",
    "Note: I'm being intentionally vague here about \"algorithm\" because there are many such algorithms. Specific algorithms are outside the scope of this workshop, but if you're interested, feel free to talk to me about it afterwards!\n",
    "\n",
    "## Model Evaluation\n",
    "You don't give your models homework for practice (in the form of training data) for no reason. Eventually, you'll want to evaluate to see how well your model performs. Just as (most) teachers wouldn't give you the answer key to the test, the model should be given new data *without* labels to see how well it performs with respect to the true labels.\n",
    "\n",
    "Generally, you may hear folks talk about 3 datasets:\n",
    "\n",
    "* Training: this is what your model sees and learns from\n",
    "* Validation (AKA dev): this is what you use to help guide your model selection choices\n",
    "* Testing: this is what you use to estimate the final performance of your model.\n",
    "\n",
    "In this workshop, we'll forget about a test set for simplicity. But in real world usage -- it's an important component!\n",
    "\n",
    "Just like a real test, the model gets a score. But we don't improve our model from this score like before. This score tells us a bit about how \"good\" our model is.\n",
    "\n",
    "This score concept keeps coming up; how do we do that? Usually with what we call a loss function. The loss function should give a notion of the \"wrongness\" of your model's prediction.\n",
    "\n",
    "There are many qualities you should evaluate in a model that are hard to summarize in a single number, though!\n",
    "\n",
    "* Does the model perform well overall?\n",
    "* Does the model have any egregious predictions (e.g. what edge cases are there?)\n",
    "* Does the model treat various groups fairly?\n",
    "* Does the model have desired system performance characteristics?\n",
    "\n",
    "\n",
    "## Deployment\n",
    "Once you decide your model's evaluation score is good enough, you might decide to deploy and use it for some application! Just like any other code, be wary of bugs -- they can be oh-so-subtle in machine learning. With great power, comes a lot of testing.\n",
    "\n",
    "If you want to deploy something we do today -- I recommend using something like [Algorithmia](https://algorithmia.com/developers/model-deployment/scikit) as it will be super easy to get something up and running quickly!\n",
    "\n",
    "## Continued Improvement\n",
    "After deploying a model, you often want to monitor it to make sure it maintains performance. There are tons of considerations here -- there's only so much we can go through in our limited time together! But a quick summary is: take a lot of what you validate before deployment and do that on a continuous basis!\n",
    "\n",
    "\n",
    "## Workshop\n",
    "We'll be looking at breast cancer data taken from the UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29 [1].\n",
    "This dataset provides several features about tumors:\n",
    "\n",
    "1. Sample code number: id number \n",
    "2. Clump Thickness: 1 - 10 \n",
    "3. Uniformity of Cell Size: 1 - 10 \n",
    "4. Uniformity of Cell Shape: 1 - 10 \n",
    "5. Marginal Adhesion: 1 - 10 \n",
    "6. Single Epithelial Cell Size: 1 - 10 \n",
    "7. Bare Nuclei: 1 - 10 \n",
    "8. Bland Chromatin: 1 - 10 \n",
    "9. Normal Nucleoli: 1 - 10 \n",
    "10. Mitoses: 1 - 10 \n",
    "11. Class: (2 for benign, 4 for malignant)\n",
    "\n",
    "A description of these parameters can be found below [2]:\n",
    "```\n",
    "Clump thickness: Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers. \n",
    "\n",
    "Uniformity of cell size/shape: Cancer cells tend to vary in size and shape. That is why these parameters are valuable in determining whether the cells are cancerous or not. \n",
    "\n",
    "Marginal adhesion: Normal cells tend to stick together. Cancer cells tends to loos this ability. So loss of adhesion is a sign of malignancy. \n",
    "\n",
    "Single epithelial cell size: Is related to the uniformity mentioned above. Epithelial cells that are significantly enlarged may be a malignant cell. \n",
    "\n",
    "Bare nuclei: This is a term used for nuclei that is not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumours. \n",
    "\n",
    "Bland Chromatin: Describes a uniform \"texture\" of the nucleus seen in benign cells. In cancer cells the chromatin tend to be more coarse. \n",
    "\n",
    "Normal nucleoli: Nucleoli are small structures seen in the nucleus. In normal cells the nucleolus is usually very small if visible at all. In cancer cells the nucleoli become more prominent, and sometimes there are more of them.\n",
    "```\n",
    "\n",
    "We'll dig into this dataset and try to predict whether a tumor is benign or malignant based on these features. It's often desirable to immediately start throwing models at the data, but the first step is always to understand your data, so let's take a look!\n",
    "\n",
    "Note: I have made some slight modifications to the dataset to make it easier to use.\n",
    "\n",
    "---\n",
    "[1] O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "[2] https://answers.yahoo.com/question/index?qid=20101204013824AAWTufG\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('breast_cancer.csv')\n",
    "\n",
    "data_tr, data_va = train_test_split(data, test_size=0.3)\n",
    "\n",
    "\n",
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, it seems clear that we just have numerical features -- that will make our first attempt at a model easier as many ML libraries expect numeric features (e.g. if we had text, we may need to convert to a numeric representation for some libraries/models).\n",
    "\n",
    "Let's take a look at some other qualities of this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that the only feature that has any missing values is `Bare Nuclei`. Proper handling of missing values can be difficult at times because depending on [how it is missing, we have to adjust our strategy for how to deal with it](https://www.theanalysisfactor.com/missing-data-mechanism/).\n",
    "\n",
    "(A common theme in good data science work: know your assumptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, we can see most of these features range from 1-10. But we also see differences in the distributions (see the rows `25%`, `50%`, and `75%`). Some of these features seem to have a lot of data on the same values (e.g. if both `25%` and `50%` have the same value, then you can be pretty sure at least 25% of the data has that value!). Also of interest is we can see a little insight into our label distribution (there are only 2 labels, `2` and `4`). It looks like it's reasonably balanced -- can you figure out the proportion of each label yourself?\n",
    "\n",
    "Let's look at how all these correlate with each other to see how they relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the diagonol is 1.0 -- this is because a variable is always perfectly correlated with itself. We can also see each variable's correlation with the class. We can see that `Bare Nuceli`, `Uniformity of Cell Size`, and `Uniformity of Cell Shape`. We can also recognize that the latter 2 are highly correlated with each other.\n",
    "\n",
    "What does that mean? It means that most of the information we could learn from one of them is provided by the other. It's tempting to always throw as many features as possible at a machine learning model (and in hackathon mode, that might be the right thing to ship quickly). But when building a real product for people, simple is often better :).\n",
    "\n",
    "I think we've seen enough to give a quick go. Let's create a first model on the `Uniformity of Cell Size` feature. We'll start out by applying a model called LogisticRegression. LogisticRegression essentially tries to divide classes of points (in our case benign vs. malignant tumors) with a line:\n",
    "\n",
    "<img src=http://mlpy.sourceforge.net/docs/3.5/_images/elasticnetc.png>\n",
    "\n",
    "The algorithm starts with an initial line and then just slightly rotates and moves until it can separate the two classes such that e.g. benign tumors are on one side of the line and malignant tumors are on the other side of the line. Luckily with scikit-learn, training such a model is easy.\n",
    "\n",
    "Do note that in general there are many different models that can solve this problem well. It's often hard to decide what model to use. In general, you should try many different kinds of models, but a good place to start is [sk learn's guide](http://scikit-learn.org/stable/tutorial/machine_learning_map/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def featurize(data, feature_columns, target_column):\n",
    "    \"\"\"Extract features/labels from a given dataset.\"\"\"\n",
    "    return data[feature_columns], data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Class\"\n",
    "feature_columns = [\"Uniformity of Cell Size\"]\n",
    "\n",
    "Xtr, Ytr = featurize(data_tr, feature_columns, target_column)\n",
    "model = LogisticRegression().fit(Xtr, Ytr)\n",
    "print(\"Train Score: {}\".format(model.score(Xtr, Ytr)))\n",
    "\n",
    "Xva, Yva = featurize(data_va, feature_columns, target_column)\n",
    "print(\"Validation Score: {}\".format(model.score(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Already have a pretty high score on our held out set. We could call it done here. But let's dig into what we're getting wrong a little bit to understand. First, let's take a look at a finer grain metric: confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def build_cm(model, data, feature_columns, target_column):\n",
    "    \"\"\"Build a confusion matrix for given model / data.\n",
    "    \n",
    "    rows -> true label\n",
    "    columns -> predicted label\n",
    "    \n",
    "    First value -> 2, Second value -> 4\n",
    "    \"\"\"\n",
    "    X, Y = featurize(data, feature_columns, target_column)\n",
    "    predictions = model.predict(X)\n",
    "    return metrics.confusion_matrix(Y, predictions, labels=[2, 4])\n",
    "\n",
    "\n",
    "cm = build_cm(model, data_tr, feature_columns, target_column)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the model has a lot more of the label `2` (~3x as much). Often imbalance in label can hurt the learning process as the model may learn to just predict the stronger class more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = build_cm(model, data_va, feature_columns, target_column)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"Num True Labels [2, 4]: [{tn + fp}, {fn + tp}]\")\n",
    "print(f\"Num Predicted Labels [2, 4]: [{tn + fn}, {tp + fp}]\")\n",
    "print(f\"Precision: {tp / (tp + fp)}\")\n",
    "print(f\"Recall / Sensitivity: {tp / (tp + fn)}\")\n",
    "print(f\"Specificity: {tn / (tn + fp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statistics tell us that our model may be biased to predicting a negative label. As such, our model is still having some trouble identifying people with malignant tumors. Let's take a closer look at the model's mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_misclassified(model, data, feature_columns, target_column):\n",
    "    \"\"\"Build dataset of misclassified rows for a model and dataset.\"\"\"\n",
    "    X, Y = featurize(data, feature_columns, target_column)\n",
    "    predictions = model.predict(X)\n",
    "    index = predictions != Y\n",
    "    \n",
    "    misclassified = data[index].copy()\n",
    "    misclassified[\"predicted_class\"] = predictions[index]\n",
    "    return misclassified\n",
    "\n",
    "build_misclassified(model, data_tr, feature_columns, target_column).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_misclassified(model, data_tr, feature_columns, target_column)\n",
    "m[m[\"Class\"] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we see pretty quickly is there are a lot of rows that have `Bare Nuclei` value of 10 -- same with a couple of other features. Let's see if there's a trend we can possibly exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[m[\"Class\"] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No values of 10 in the other mispredictions. Also, we notice that `Bare Nuclei` has NaN values here. Let's take a look at some of our statistics from earlier broken down by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[data_tr[\"Class\"] == 2].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[data_tr[\"Class\"] == 2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[data_tr[\"Class\"] == 4].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr[data_tr[\"Class\"] == 4].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are huge differences in the mean for most of these features, and it may be that `Bare Nuclei` isn't missing completely at random -- it seems to be missing at a much higher rate for benign tumors.\n",
    "\n",
    "We should then be careful with its use as a feature. If we were to fill its value, what should we fill it with? There are [many options](https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation). We'll do the naive thing first and give it a sentinel value of -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "target_column = \"Class\"\n",
    "feature_columns = [\"Bare Nuclei\", \"Uniformity of Cell Size\"]\n",
    "\n",
    "Xtr, Ytr = featurize(data_tr, feature_columns, target_column)\n",
    "model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "    ('logreg', LogisticRegression())\n",
    "]).fit(Xtr, Ytr)\n",
    "\n",
    "print(\"Train Score: {}\".format(model.score(Xtr, Ytr)))\n",
    "\n",
    "Xva, Yva = featurize(data_va, feature_columns, target_column)\n",
    "print(\"Validation Score: {}\".format(model.score(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We have now brought our score up a lot -- let's take a deeper look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cm = build_cm(model, data_va, feature_columns, target_column)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(cm)\n",
    "print(f\"Num True Labels [2, 4]: [{tn + fp}, {fn + tp}]\")\n",
    "print(f\"Num Predicted Labels [2, 4]: [{tn + fn}, {tp + fp}]\")\n",
    "print(f\"Precision: {tp / (tp + fp)}\")\n",
    "print(f\"Recall / Sensitivity: {tp / (tp + fn)}\")\n",
    "print(f\"Specificity: {tn / (tn + fp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot better!\n",
    "\n",
    "There may be more ways to improve this model -- now it's your turn. What else should we do to improve this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
